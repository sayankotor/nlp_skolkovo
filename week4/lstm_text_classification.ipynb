{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identification of spam text messages\n",
    "\n",
    "_partially based on [this tutorial](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb)_\n",
    "\n",
    "We have a dataset of SMS in English labelled as malicious (\"spam\") or useful (\"ham\"). We will use a bi-directional RNN with LSTM cells to classify them.\n",
    "\n",
    "**Table of contents**:\n",
    "1. [Data preparation](#data_preparation)\n",
    "2. [Model description](#model_description)\n",
    "3. [Model training](#model_training)\n",
    "4. [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from nltk import wordpunct_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_preparation'></a>\n",
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-20 15:57:47--  https://drive.google.com/uc?export=download&id=1OcasoyGjTRh9xBhtRnmvVLjcJaXW4uTC\n",
      "Resolving drive.google.com (drive.google.com)... 64.233.162.194, 2a00:1450:4010:c05::c2\n",
      "Connecting to drive.google.com (drive.google.com)|64.233.162.194|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0g-58-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/lg5e3ucairnuak4e3gebvjojt9qbneo1/1574258400000/06955062529639551827/*/1OcasoyGjTRh9xBhtRnmvVLjcJaXW4uTC?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2019-11-20 15:57:47--  https://doc-0g-58-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/lg5e3ucairnuak4e3gebvjojt9qbneo1/1574258400000/06955062529639551827/*/1OcasoyGjTRh9xBhtRnmvVLjcJaXW4uTC?e=download\n",
      "Resolving doc-0g-58-docs.googleusercontent.com (doc-0g-58-docs.googleusercontent.com)... 64.233.165.132, 2a00:1450:4010:c08::84\n",
      "Connecting to doc-0g-58-docs.googleusercontent.com (doc-0g-58-docs.googleusercontent.com)|64.233.165.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 477907 (467K) [text/plain]\n",
      "Saving to: ‘sms_spam.txt’\n",
      "\n",
      "sms_spam.txt        100%[===================>] 466.71K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2019-11-20 15:57:48 (4.44 MB/s) - ‘sms_spam.txt’ saved [477907/477907]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O sms_spam.txt \"https://drive.google.com/uc?export=download&id=1OcasoyGjTRh9xBhtRnmvVLjcJaXW4uTC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and get the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "def load_data(path):\n",
    "    vocab = set()\n",
    "    text = []\n",
    "    labels = []\n",
    "    label_dict = {'ham': [1, 0], 'spam': [0, 1]}\n",
    "    for line in open(path):\n",
    "        [label, txt] = line.lower().strip('\\n').split('\\t')\n",
    "        words = wordpunct_tokenize(txt)\n",
    "        vocab.update(words)\n",
    "        text.append(words)\n",
    "        labels.append(label_dict[label])\n",
    "    return text, labels, vocab\n",
    "txt, labels, vocab = load_data('sms_spam.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the previous experiments, here we will use pre-trained word embeddings (GloVe).\n",
    "\n",
    "Let us get the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-20 14:27:10--  https://91.77.165.184:8890/lab/tree/glove.6B.100d.tar.xz\n",
      "Connecting to 91.77.165.184:8890... connected.\n",
      "WARNING: cannot verify 91.77.165.184's certificate, issued by ‘emailAddress=rsuvorov@isa.ru,CN=exnibm0.isa.ru,OU=Lab 07,O=FRC CSC RAS,L=Moscow,ST=Moscow,C=RU’:\n",
      "  Self-signed certificate encountered.\n",
      "    WARNING: certificate common name ‘exnibm0.isa.ru’ doesn't match requested host name ‘91.77.165.184’.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /login?next=%2Flab%2Ftree%2Fglove.6B.100d.tar.xz [following]\n",
      "--2019-11-20 14:27:10--  https://91.77.165.184:8890/login?next=%2Flab%2Ftree%2Fglove.6B.100d.tar.xz\n",
      "Reusing existing connection to 91.77.165.184:8890.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6677 (6.5K) [text/html]\n",
      "Saving to: ‘glove100.6B.100d.tar.xz’\n",
      "\n",
      "glove100.6B.100d.ta 100%[===================>]   6.52K  --.-KB/s    in 0s      \n",
      "\n",
      "2019-11-20 14:27:10 (993 MB/s) - ‘glove100.6B.100d.tar.xz’ saved [6677/6677]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the embeddings and transform the vocabulary so that it includes only words that occur in the data and for which embeddings exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the embeddings\n",
    "def load_embeddings(emb_path, vocab):\n",
    "    clf_embeddings = {}\n",
    "    emb_vocab = set()\n",
    "    for line in open(emb_path):\n",
    "        line = line.strip('\\n').split()\n",
    "        word, emb = line[0], line[1:]\n",
    "        emb = [float(e) for e in emb]\n",
    "        if word in vocab:\n",
    "            clf_embeddings[word] = emb\n",
    "    for w in vocab:\n",
    "        if w in clf_embeddings:\n",
    "            emb_vocab.add(w)\n",
    "    word2idx = {w: idx for (idx, w) in enumerate(emb_vocab)}\n",
    "    max_val = max(word2idx.values())\n",
    "    \n",
    "    word2idx['UNK'] = max_val + 1\n",
    "    word2idx['EOS'] = max_val + 2\n",
    "    emb_dim = len(list(clf_embeddings.values())[0])\n",
    "    clf_embeddings['UNK'] = [0.0 for i in range(emb_dim)]\n",
    "    clf_embeddings['EOS'] = [0.0 for i in range(emb_dim)]\n",
    "    \n",
    "    embeddings = [[] for i in range(len(word2idx))]\n",
    "    for w in word2idx:\n",
    "        embeddings[word2idx[w]] = clf_embeddings[w]\n",
    "    embeddings = torch.Tensor(embeddings)\n",
    "    return embeddings, word2idx\n",
    "embeddings, vocab = load_embeddings('../../glove.6B.100d.txt', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the lengths of sentences in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average SMS length:  19.554897739504845\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "txt_lengths = [len(t) for t in txt] \n",
    "print('Average SMS length: ', np.average(txt_lengths))\n",
    "p = plt.hist(txt_lengths, bins=30)\n",
    "l = plt.xlabel('# words per text')\n",
    "l = plt.ylabel('# texts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the majority of sentences are not longer than 50 words, we can truncate longer texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = [t[:50] for t in txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = plt.hist([len(t) for t in txt] , bins=30)\n",
    "l = plt.xlabel('# words per text')\n",
    "l = plt.ylabel('# texts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform a list of lines of different lengths to a matrix which can be used as an input to a neural network.\n",
    "\n",
    "We pad the shorter lines with EOS symbol to match the length of the longest line in a batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_matrix(lines, vocab, max_len=None, dtype='int32'):\n",
    "    \"\"\"Casts a list of lines into a matrix\"\"\"\n",
    "    pad = vocab['EOS']\n",
    "    max_len = max_len or max(map(len, lines))\n",
    "    lines_ix = np.zeros([len(lines), max_len], dtype) + pad\n",
    "    for i in range(len(lines)):\n",
    "        line_ix = [vocab.get(l, vocab['UNK']) for l in lines[i]]\n",
    "        lines_ix[i, :len(line_ix)] = line_ix\n",
    "    lines_ix = torch.LongTensor(lines_ix)\n",
    "    return lines_ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the performance of this procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4307, 6660, 6660],\n",
      "        [4307, 3458, 6660],\n",
      "        [1082, 3832, 6179],\n",
      "        [6659, 6659, 6660]])\n"
     ]
    }
   ],
   "source": [
    "dummy_lines = [\n",
    "    ['one'],\n",
    "    ['one', 'two'],\n",
    "    ['the', 'long', 'line'],\n",
    "    ['unkn0wn', 'w0rds']\n",
    "]\n",
    "print(to_matrix(dummy_lines, vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the train/validation/test partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = [], [], []\n",
    "for idx, (t, l) in enumerate(zip(txt, labels)):\n",
    "    t = to_matrix([t], vocab)\n",
    "    l = torch.Tensor([l])\n",
    "    if idx % 10 == 0:\n",
    "        valid_data.append((t, l))\n",
    "    elif (idx+1) % 10 == 0:\n",
    "        test_data.append((t, l))\n",
    "    else:\n",
    "        train_data.append((t, l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model_description'></a>\n",
    "### Model\n",
    "\n",
    "Let us now define our model. It will consist of:\n",
    "- an embedding layer which will use pre-trained GloVe embeddings with no fine-tuning\n",
    "- a layer of LSTM cells\n",
    "- a dense layer that will convert the LSTM output to scores of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_dim=128, lstm_layer=1, output=2):\n",
    "        \n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # load pre-trained embeddings\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings)\n",
    "        # embeddings are not fine-tuned\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # RNN layer with LSTM cells\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding.embedding_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=lstm_layer, \n",
    "                            bidirectional=True)\n",
    "        # dense layer\n",
    "        self.output = nn.Linear(hidden_dim*2, output)\n",
    "    \n",
    "    def forward(self, sents):\n",
    "        x = self.embedding(sents)\n",
    "        \n",
    "        # the original dimensions of torch LSTM's output are: (seq_len, batch, num_directions * hidden_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # reshape to get the tensor of dimensions (seq_len, batch, num_directions, hidden_size)\n",
    "        lstm_out = lstm_out.view(x.shape[0], -1, 2, self.hidden_dim)#.squeeze(1)\n",
    "        \n",
    "        # lstm_out[:, :, 0, :] -- output of the forward LSTM\n",
    "        # lstm_out[:, :, 1, :] -- output of the backward LSTM\n",
    "        # we take the last hidden state of the forward LSTM and the first hidden state of the backward LSTM\n",
    "        dense_input = torch.cat((lstm_out[-1, :, 0, :], lstm_out[0, :, 1, :]), dim=1)\n",
    "        \n",
    "        y = self.output(dense_input).view([1, 2])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us initialise the model, optimiser, and loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 128\n",
    "layers = 1\n",
    "\n",
    "model = BiLSTM(embeddings, hidden_dim, lstm_layer=layers)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of trainable parameters in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 236,034 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer the model and loss to GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides loss, we also need a metric which is interpretable by humans. We will use accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    # y is either [0, 1] or [1, 0]\n",
    "    # get the class (0 or 1)\n",
    "    y = torch.argmax(y, dim=1)\n",
    "    \n",
    "    # get the predicted class\n",
    "    preds = torch.argmax(torch.sigmoid(preds), dim=1)\n",
    "    \n",
    "    correct = (preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model_training'></a>\n",
    "### Training of a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure to perform one epoch of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    # set the model to the training mode\n",
    "    model.train(mode=True)\n",
    "    \n",
    "    for t, l in train_data:\n",
    "        # reshape the data to n_words x batch_size (here batch_size=1)\n",
    "        t = t.view((-1, 1))\n",
    "        # transfer the data to GPU to make it accessible for the model and the loss\n",
    "        t = t.to(device)\n",
    "        l = l.to(device)\n",
    "        \n",
    "        # set all gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass of training\n",
    "        # compute predictions with current parameters\n",
    "        predictions = model(t)\n",
    "        # compute the loss\n",
    "        loss = criterion(predictions, l)\n",
    "        # compute the accuracy (this is only for report)\n",
    "        acc = binary_accuracy(predictions, l)\n",
    "        \n",
    "        # backward pass (fully handled by pytorch)\n",
    "        loss.backward()\n",
    "        # update all parameters according to their gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # data for report\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(train_data), epoch_acc / len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation procedure computes loss and accuracy, but does not update gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for t, l in test_data:\n",
    "            t = t.view((-1, 1))\n",
    "            t = t.to(device)\n",
    "            l = l.to(device)\n",
    "            predictions = model(t)\n",
    "            \n",
    "            loss = criterion(predictions, l)\n",
    "            acc = binary_accuracy(predictions, l)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(test_data), epoch_acc / len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedure to compute the time between *start_time* and *end_time* points (we'll use it to note the time during training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training and evaluation for several epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.432 | Train Acc: 86.79%\n",
      "\t Val. Loss: 0.397 |  Val. Acc: 84.05%\n",
      "Epoch: 02 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.347 | Train Acc: 87.17%\n",
      "\t Val. Loss: 0.386 |  Val. Acc: 84.05%\n",
      "Epoch: 03 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.335 | Train Acc: 87.17%\n",
      "\t Val. Loss: 0.371 |  Val. Acc: 84.05%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_data, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_data, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercises'></a>\n",
    "### Exercises\n",
    "\n",
    "1. Change the parameters of model, find the best configuration:\n",
    " - add another layer,\n",
    " - increase the number of cells in the hidden layer,\n",
    " - change LSTM cells to GRU.\n",
    "2. Compute F$_1$-score on the validation set.\n",
    "3. Use <code>torchtext</code> library to load the text and generate batches.\n",
    "4. \\*Change the model so that it can solve the regression task. You can use reviews from the [Movie\\$ Data corpus](http://www.cs.cmu.edu/~ark/movie$-data/) or the [dataset](http://hdl.handle.net/11372/LRT-2619) for sentence-level quality estimation task.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
